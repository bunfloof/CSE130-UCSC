# Chap01-05_Reviews Non-human

published: No

## 💦 Chapter01 Introduction Review

### 1. Explain the main purposes of an operating system?

- (1) As an **abstraction**, the operating system’s job is to provide the users with abstractions, such as processes, address spaces, and files, which are more convenient to use than the actual hardware,
- (2) As a **resource manager**, the operating system’s job is to manage the different parts of the system efficiently.

### 2. What is time sharing in OS? What is space sharing in OS?

- Time sharing: sharing executing power (such as **CPU**, logical processor, GPU) by many users (such as OS processes, threads, network requests) at the **same time**.
- Space sharing: **sharing memory space** (hard disk, RAM, database) by many different users (such as inplace algorithms, executing threads) at the **same space**.

### 3. What is a trap instruction? Explain its use in operating systems.

- A Trap is a **software generated interrupt**. A Trap is set to have occurred when some exceptions occurs like - a process accessing a memory address outside it's address space would result in a trap which is handled by the OS.
- A trap usually results in a **switch from user mode to kernel mode**, wherein the operating system performs some action before returning control to the originating process.

### 4. What are interrupts?

- An interrupt is something generated by **the hardware** (devices like the hard disk, graphics card, I/O ports, etc). These are asynchronous (i.e. they don't happen at predictable places in the user code) or "passive" since **the interrupt handler has to wait for them to happen eventually**.

### 5. What is CPU pipeline?

- A technique that implements a form of parallelism called **instruction-level parallelism** within a single processor. It therefore allows **faster CPU throughput** (the number of instructions that can be executed in a unit of time) than would otherwise be possible at a given clock rate.

### 6. What is virtual memory and virtual memory address?

- Virtual memory is a feature of an operating system that allows a computer to compensate for shortages of physical memory by temporarily transferring pages of program data from main memory to disk storage.
- It maps memory addresses used by a program, called virtual addresses, into physical addresses in computer memory.

### 7. What is i-Node? Explain its use in operating systems.

- i-Node, one per file, a data structure used to represent a filesystem object, telling who owns the file, where its disk blocks are, and so on

## 💦 Chapter02 Process Review

### 1. What is a process?

- A process is an **instance** of a running program. It contains the program code and its current activity, such as **registers, variables, program counter, input, output and a state**.

### 2. What is a thread?

- A thread is **the entity within a process** that can be scheduled for execution.

### 3. What is the difference between a process and a thread?

- Threads are used for **`lightweight’** tasks, whereas processes are used for more **'heavyweight'** tasks.
- The typical difference is that threads (of the same process) run in a **shared memory space**, while processes run in **separate memory spaces**.

### 4. What is context switching?

- Transferring **the control from one process to other process** requires **saving** the state of the old process and **loading** the saved state for new process. This task is known as context switching.

### 5. What are the disadvantages of context switching?

- **Time taken for switching from one process to other is pure overhead.** The system does no useful work while switching, so one of the solutions is to go for threading when ever possible

### 6. What is race condition?

- A situation, **where several processes access and manipulate the same data concurrently and the outcome of the execution depends on the particular order in which the access takes place**, is called race condition.

### 7. What is process synchronization?

- To guard against the race condition we need to ensure that only one process at a time can be manipulating the same data. The technique we use for this is called **process synchronization**.

### 8. What is busy waiting?

- The **repeated** execution of a loop of code while **waiting for an event to occur** is called busy waiting.

### 9. What is mutex?

- A mutex is a **lock**. It is **a program object** that allows multiple program threads to share the same resource, such as file access, **but not simultaneously**.
- When a program is started, a mutex is created with a unique name. After this stage, any thread that needs the resource must lock the mutex from other threads while it is using the resource. The mutex is set to unlock when the data is no longer needed or the routine is finished.

### 10. What is a semaphore?

- It is a **synchronization** tool used to solve **complex critical section problems**. A semaphore is an integer variable that, apart from initialization, is accessed only through two standard **atomic** operations: **down, and up**.
- **Down checks semaphore**. If not zero, decrements semaphore. If zero, process goes to sleep
- **Up increments semaphore**. If more then one process asleep, one is chosen randomly and enters critical region (first does a down)

### 11. What is the difference between mutex and semaphore?

- Mutex is an object owned by thread, so there is a **ownership** in mutex. **Mutex allow only one thread to access resource**.
- Semaphore is a **signaling mechanism**. It allows **a number of thread to access shared resources.**

### 12. What is throughput, turnaround time, waiting time and Response time?

- **Throughput**: number of processes that complete their execution per time unit.
- **Turnaround time**: amount of time to execute a particular process.
- **Waiting time**: amount of time a process has been waiting in the ready queue.
- **Response time**: amount of time it takes from when a request was submitted until the first response is produced, not output (for time-sharing environment)

### 13. What are the scheduling algorithm goals for batch, interactive, real-time operating systems?

**All systems**

- Fairness - giving each process a fair share of the CPU
- Policy enforcement - seeing that stated policy is carried out
- Balance - keeping all parts of the system busy

**Batch systems** 

- Throughput - maximize jobs per hour
- Turnaround time - minimize time between submission and termination
- CPU utilization - keep the CPU busy all the time
- Interactive systems Response time - respond to requests quickly
- Proportionality - meet users' expectations

**Real-time systems**

- Meeting deadlines - avoid losing data
- Predictability - avoid quality degradation in multimedia systems

### 14. Name at least four the different job scheduling in operating systems?

Scheduling is the activity of the deciding when process will receive the resources they request. 

- FCFS ---> FCSFS stands for First Come First Served. In FCFS the job that has been waiting the longest is served next.
- Round Robin Scheduling--->Round Robin scheduling is a scheduling method where each process gets a small quantity of time to run and then it is preempted and the next process gets to run. This is called time-sharing and gives the effect of all the processes running at the same time
- Shortest Job First ---> The Shortest job First scheduling algorithm is a nonpreemptive scheduling algorithm that chooses the job that will execute the shortest amount of time.
- Priority Scheduling--->Priority scheduling is a scheduling method where at all times the highest priority process is assigned the resource.

## 💦 Chapter03 Deadlocks Review

### 1. What are four conditions must exist in order for deadlock to occur?

For a set of processes to be deadlocked: (1) at least one resource must remain in a **non-sharable** mode, (2) a process must **hold at least one resource and be waiting** to acquire additional resources held by other processes, (3) resources in the system **can not be preempted**, and (4) a **circular wait** has to exist between processes.

### 2. What are the four strategies to deal with deadlock?

(1) An operating system may just **ignore** the problem and pretend that deadlocks can never occur.

(2) A system may allow a deadlock to occur, **detect** it, and **recover** from it.

(3) A system may perform **dynamic avoidance** for deadlocks by careful resource allocation.

(4) A deadlock can be prevented by structurally **negating one of the four required conditions**.

### 3. How might an operating system attack the 1st , 2nd, 3rd, 4th condition necessary for deadlock in order to prevent the problem of deadlock?

| Condition | Approach |
| --- | --- |
| Mutual exclusion | Spool everything |
| Hold and wait | Request all resources intitially |
| No preemption | Take resources away |
| Circular wait | Order resources numerically |

**More details**

- **Spool everything**: making resource sharable
- **Request all resources initially**: requiring processes to request all of their resources at the same time.
- **No preemption**: The operating system may forcibly deallocate resources from deadlocked processes thus attacking the condition of no preemption.
- **Order resources numerically**: impose a total ordering of all resource types, and to require that each process requests resources in an increasing order of enumeration. This can be Order resources numerically: impose a total ordering of all resource types, and to require that each process requests resources in an increasing order of enumeration. This can be accomplished by assigning each resource type a unique integer number to determine whether one precedes another in the ordering

### 4. What is the difference between deadlock prevention and deadlock avoidance?

Deadlock prevention is a set of methods for **ensuring that at least one of the necessary conditions for deadlock can not hold**. Deadlock avoidance requires that the operating system be given, in advance, **additional information concerning which resources a process** will request and use during its lifetime.

### 5. A computer has six tape drives, with n processes competing for them. Each process may need up to two drives. For which values of n is the system guaranteed to be deadlock free? Explain

If n <= 3, then clearly there can be no deadlock, since even in the worst case, if 3 processes all simultaneously get a tape drive, there is still enough to go around. But note that even for n = 5, there can't be any deadlock. In the worst case scenario with n = 5, all five will get one tape drive, and there is still an extra that can be given to any single process. That process would then finish, freeing up two tape drives for the remaining four, etc. Alternatively, just create a banker's algorithm table with 5 processes, max=2 for each of them, and check that no possible allocations of six tape drives will be unsafe. So n < 6  or n <= 5 is our final answer.

## 💦 Chapter04 Memory Review

### 1. List and describe the four memory allocation algorithms covered in lectures.

- **First-Fit** – in the linked list of available memory addresses, we place the data in the first entry that will fit its data. Its aim is to minimize the amount of searching, but leads to external fragmentation later on.
- **Next-Fit** – similar to first fit, but instead of searching from the beginning each time, it searches from the last successful allocation. Greatly reduces the amount of searching but leaves external fragmentation at the beginning of memory.
- **Worst-Fit** – traverses the memory and gives the partitions as large spaces as possible – to leave usable fragments left over. Needs to search the complete list and such is a poor performer.
- **Best-Fit** – carefully scours the memory for spaces that perfectly fit the RAM we want. However, the search is likely to take a very long time.

### 2. What is the basic function of paging?

Paging is a memory management scheme that permits **the physical-address space of a process to be noncontiguous**. It avoids the considerable problem of having to **fit varied sized memory chunks onto the backing store**.

### 3. One of the design decisions in OS memory management is the choice between swapping and paging. Define each of these terms, and clarify their respective roles in OS memory management.

- swapping: **copies entire process image between memory and disk**. Assumes contiguous allocation, entire process needed for execution. Used to limit multiprogramming level and avoid thrashing.
- paging: **divides** logical address space of process into **fixed-size pieces**; process can execute with only a **subset** of these pages being resident in memory at a time. Provides flexible and efficient memory management, with lots of processes active at a time.

### 4. Describe page-based virtual memory. You should consider pages, frames, page tables, and Memory Management Units in your answer.

- Page based virtual memory ensures that every process has **its own address space**. It allows processes access up to 2^(system bits) bytes of address space, and **allows programs requiring more RAM to use only what it needs to at any one point**. Physical memory need not be contiguous. **No external fragmentation**. Minimal internal fragmentation. **Allows sharing** (you can map several pages to the same frame)

### 5. What is a translation look-aside buffer? What is contained in each entry it contains?

- A **translation look-aside buffer (TLB)** is a high speed **cache** for **page table entries** (that can either be virtual or part of the physical MMU) containing those that are recently used. It consists of two different entries, EntryLo and EntryHi. EntryHi contains a **virtual address**. EntryLo contains **a corresponding physical address and a number of bits signifying whether the address is 'dirty', 'empty' or otherwise**.

### 6. Describe how OS deals with a page fault?

![Untitled](Chap01-05_Reviews%20Non-human%20a361c7c67202483386714a52f5babf37/Untitled.png)

### 7. Enumerate some pros and cons for increasing the page size.

Pros:

- **Reduce total page table size**, freeing some memory
- **Increase TLB coverage** - Increase swapping I/O throughput

Cons:

- **Increase page fault latency** (more page to search through)
- **Increase internal fragmentation of pages**

### 8. What is the working set of a process?

The working set of a process is the **allocated pages/segments at any one time window** (delta), consisting of all pages accessed during that time. Includes **current top of stack, areas of the heap, current code segment and shared libraries**.

### 9. How does page size of a particular architecture affect working set size?

Page size of a particular architecture affects working set size because **the larger the pages, the more memory that can be wasted on irrelevant data**, vastly increasing the working set size for no good reason. If they’re smaller, the pages accurately reflect current memory usage.

### 10. In pure on-demand paging, a page replacement policy is used to manage system resources. Suppose that a newly-created process has 3 page frames (A, B, C) allocated to it, and then generates the page references indicated below.

(i) How many page faults would occur with FIFO page replacement? Explain.

**A B C B A D A B C D A B A C B D** 

**Answer: 12**

Here’s a table representation of the process:

| Pages | A | B | C | B | A | D | A | B | C | D | A | B | A | C | B | D |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| f_1 | A | A | A | A | A | D | D | D | C | C | C | B | B | B | B | B |
| f_2 |  | B | B | B | B | B | A | A | A | D | D | D | D | C | C | C |
| f_3 |  |  | C | C | C | C | C | B | B | B | A | A | A | A | A | D |
|  | X | X | X | ✓ | ✓ | X | X | X | X | X | X | X | ✓ | X | ✓ | X |

(ii) How many page faults would occur with LRU page replacement?

**A B C B A D A B C D A B A C B D**

********************Answer: 10********************

Here’s a table representation of the process:

| Pages | A | B | C | B | A | D | A | B | C | D | A | B | A | C | B | D |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| f_1 | A | A | A | A | A | A | A | A | A | D | D | D | D | C | C | C |
| f_2 |  | B | B | B | B | B | B | B | B | B | A | A | A | A | A | D |
| f_3 |  |  | C | C | C | D | D | D | C | C | C | B | B | B | B | B |
|  | X | X | X | ✓ | ✓ | X | ✓ | ✓ | X | X | X | X | ✓ | X | ✓ | X |

## 💦 Chapter05 File Systems Review

### 1. What are the advantages and disadvantages of continuous allocation of free blocks to files?

- The good:
    - **Easy to implement**
    - **Read performance is great**. Only need one seek to locate the first block in the file. The rest is easy.
- The bad:
    - **Disk becomes fragmented over time**
    - It is necessary to know the **file's final size** prior to even allocating any space

### 2. Give an example where contiguous allocation of file blocks on disks can be used in practice.

- When you're **writing a memory dump** to disk because your OS crashed
- It is still used on **write-once optical media**, because prior to **burning your CD, DVD or BD** the system knows exactly how much space each file uses on said disk.

### 3. What are the advantages and disadvantages of linked-list allocation of free blocks to files?

- The good
    - Gets rid of fragmentation
- The bad
    - Random access is slow. Need to chase pointers to get to a specified block

### 4. What file access pattern is particularly suited to linked-list allocation of free blocks to files?

- It is fine for **sequential access** because every block needs to be read regardless. It's a living nightmare for random access files because we introduce a lot of unnecessary, wasted “read” operations to the FS.

### 5. What is i-node?

- The i-node is a data structure which **lists all attributes and points to all the addresses of the disk blocks corresponding to that file**.

### 6. What file allocation strategy is most appropriate for random access files?

- An i-node (indexed node) based file allocation strategy. It is a form of file allocation table that offers temporal locality.
- **The i-node only needs to be located in memory when its corresponding file is open.**

### 7. What is the reference count field in the inode?

- The reference count field is a counter of **how many times the i-node is “referenced” by name**.
- Adding a directory entry increments this counter. **When the count falls to zero**, (i.e. there are no longer any directory entries to that file), its i-node and all corresponding disk blocks can be safely **deallocated**.