# Assign4

published: No

## Question at position 1

List and describe the four memory allocation algorithms covered in lectures.

1. The first fit algorithm is the simplest of the four algorithms. When process needs memory, the first fit algorithm scans through the list of memory segments until it finds a "hole" (unused memory) that is big enough for the process. Then, this hole is divided into two parts, one for the process and one for the remaining unused memory. This method is fast because it minimizes search time by allocating the first suitable hole it finds.
2. The next fit algorithm is a variation of the first fit algorithm. It tracks the last allocated space, and when a new request arrives, it starts searching from the last allocated space and not from the beginning of the memory list. If it finds a memory hole that is big enough to satisfy the request, it allocates the memory from this hole. If it doesn’t find a sufficient hole after traversing the memory list once, it does not allocate memory. The disadvantage, simulations pointed out by Bays (1977), is that it gives slightly worse performance than first fit.
3. The best fit algorithm searches the entire memory list from beginning to end and chooses the smallest memory hole that is sufficient to accommodate the memory request. It tries to find a hole that is closest to the actual request size in order to minimize the wastage of memory. However, this strategy result in higher fragmentation with many tiny, useless holes in memory over time because it tends to use the smallest adequate hole each time. It’s also slower than the first fit and next fit algorithms because it searches the entire list of holes each time it’s called.
4. The worst fit algorithm always chooses the largest available memory hole for allocation. The rationale behind this strategy is that the remaining memory hole after allocation will be big enough to be useful for future allocations. However, worst fit is not a very good strategy because it can also lead to memory fragmentation and inefficient utilization of memory.
5. ~~The quick fit algorithm maintains separate lists for the most commonly requested memory sizes. For example, if processes often request 4-KB, 8-KB, and 12-KB blocks, it maintains separate lists for each of these sizes. It’s fast when a hole of the required size is needed because it can immediately reference the appropriate list. The drawback is when a process terminates/swaps out, merging its space with the neighboring holes (if they’re free) can be resource expensive. If merging is not performed, the memory can fragment into many small holes that may not fit any processes.~~

⚠️ Memory review ppt reports first fit instead of quick fit?

![Untitled](Assign4%200d31041fea15417fa3f0be86960c3795/Untitled.png)

## Question at position 2

What is a translation look-aside buffer? What is contained in each entry it contains? 

A Translation Look-aside Buffer (TLB) is hardware device designed to speed up the process of virtual-to-physical address translation during paging. The TLB is located inside the memory management unit (MMU) of a computer and serves as a cache for page table, storing recent translations of virtual memory to physical memory addresses.

The use of TLB is based on the observation that most programs tend to make a large number of references to a small number of pages. It keeps information about heavily used pages in the TLB, so that the the system can quickly translate their addresses without having to access the page table in main memory, which is a slower operation.

Each entry in the TLB contains the following information:

1. **Valid bit** indicates whether the entry is valid (in use) or not.
2. **Virtual page number** is the page number in the virtual memory space.
3. **Dirty bit** is set when the page has been modified (written to).
4. **Protection code** is the access permissions for the page (read, write, execute permissions).
5. **Physical page frame** is the corresponding physical memory address (frame number) where the page is located.

When a virtual address needs to be translated, the hardware first checks the TLB. If valid match is found and the access does not violate the protection bits, the physical page frame is taken directly from the TLB. If match isn't found, this results in TLB miss, so the system looks up the page table in memory. The new entry from the page table replaces an old one in the TLB so that future references to the new page can be quickly resolved. This replacement process usually involves an algorithm (FIFO, CLOCK, LRU, etc.) to determine which entry to replace.

## Question at position 3

Enumerate some pros and cons of increasing the page size. Enumerate some pros and cons of increasing the page size.

**Pros of increasing page size:**

1. **Reduced page table size**: Larger pages means fewer pages for the same amount of memory, which results in a smaller page table, so it improves efficiency because the page table consumes less memory.
2. **Reduced disk transfed time**:  A significant amount of time taken for disk I/O operations is for the seek and rotational delay, so transferring a large page can be faster than transferring the equivalent amount of memory in small pages.
3. **Improved TLB efficiency/lower TLB misses**: Larger pages mean fewer entries in the Translation Look-aside Buffer (TLB) for the same amount of memory. TLB entries are scarce and critical for performance, so fewer entries improve cache hit rate and overall performance.

**Cons of increasing page size:**

1. **Increased internal fragmentation**: In larger pages, there is a higher likelihood that memory will be wasted due to internal fragmentation because the final part of a page may not be fully utilized, resulting in wasted memory.
2. **Inefficient for small memory requests**: If program only needs a small amount of additional memory, it still needs to allocate a whole new page, which leads to wasted space if the page size is large.
3. **Inefficient for small processes**: If there are many small processes, they may each take up a full page even though they don't need all that space, leading to inefficient memory usage.
4. **Potentially increased page faults**: If active parts of memory (the working set) span across more pages due to a larger page size, it could lead to more page faults when the working set can not be fully loaded into available memory.

## Question at position 4

In pure on-demand paging, a page replacement policy is used to manage system resources. Suppose that a newly-created process has 3 page frames (A, B, C) allocated to it, and then generates the page references indicated below:
 
A B C B A D A B C D A B A C B D
 
How many page faults would occur with FIFO page replacement? Explain.

Here’s a table representation of the process:

| Pages | A | B | C | B | A | D | A | B | C | D | A | B | A | C | B | D |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| f_1 | A | A | A | A | A | D | D | D | C | C | C | B | B | B | B | B |
| f_2 |  | B | B | B | B | B | A | A | A | D | D | D | D | C | C | C |
| f_3 |  |  | C | C | C | C | C | B | B | B | A | A | A | A | A | D |
|  | X | X | X | ✓ | ✓ | X | X | X | X | X | X | X | ✓ | X | ✓ | X |

(Legend: X = Page Fault, ✓ = Page Hit)

As shown from the table above, there are 12 page faults. 

Here’s a chronological textual explanation:

1-3. The first three references (“A”, “B”, “C”) are page faults because these pages were not in the memory when they were first referenced, so they need to be loaded into memory.

1. On the next reference “B”, this is already in memory.
2. On the next reference “A”, this is already in memory.
3. On the next reference “D”, this is a page fault because it is not in memory. According to the FIFO (First-in first-out) policy, A, which was the first page loaded, is replaced. Now the frames contain “D”, “B”, “C”.
4. On the next reference “A”, this is a page fault because it is not in memory. According to the FIFO policy, “C”, which was the first page loaded, is replaced. Now the frames contain “D”, “A”, “C”.
5. On the next reference “B”, this is a page fault because it is not in memory. According to the FIFO policy, “C”, which was the first page loaded, is replaced. Now the frames contain “D”, “A”, “B”.
6. On the next reference “C”, this is a page fault because it is not in memory. According to the FIFO policy, “D”, which was the first page loaded, is replaced. Now the frames contain “C”, “A”, “B”.
7. On the next reference “D”, this is a page fault because it is not in memory. According to the FIFO policy, “A”, which was the first page loaded, is replaced. Now the frames contain “C”, “D”, “B”.
8. On the next reference “A”, this is a page fault because it is not in memory. According to the FIFO policy, “B”, which was the first page loaded, is replaced. Now the frames contain “C”, “D”, “A”.
9. On the next reference “B”, this is a page fault because it is not in memory. According to the FIFO policy, “C”, which was the first page loaded, is replaced. Now the frames contain “B”, “D”, “A”.
10. On the next reference “A”, this is already in memory.
11. On the next reference “C”, this is a page fault because it is not in memory. According to the FIFO policy, “D”, which was the first page loaded, is replaced. Now the frames contain “B”, “C”, “A”.
12. On the next reference “B”, this is already in memory.
13. On the next reference “D”, this is a page fault because it is not in memory. According to the FIFO policy, “A”, which was the first page loaded, is replaced. Now the frames contain “B”, “C”, “D”.

Therefore, there are 12 page faults.

⚠️ Memory review ppt reports 11 page faults?

![Untitled](Assign4%200d31041fea15417fa3f0be86960c3795/Untitled%201.png)

## Question at position 5

In pure on-demand paging, a page replacement policy is used to manage system resources. Suppose that a newly-created process has 3 page frames (A, B, C) allocated to it, and then generates the page references indicated below:

A B C B A D A B C D A B A C B D

How many page faults would occur with LRU page replacement? Explain.

Here’s a table representation of the process:

| Pages | A | B | C | B | A | D | A | B | C | D | A | B | A | C | B | D |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| f_1 | A | A | A | A | A | A | A | A | A | D | D | D | D | C | C | C |
| f_2 |  | B | B | B | B | B | B | B | B | B | A | A | A | A | A | D |
| f_3 |  |  | C | C | C | D | D | D | C | C | C | B | B | B | B | B |
|  | X | X | X | ✓ | ✓ | X | ✓ | ✓ | X | X | X | X | ✓ | X | ✓ | X |

(Legend: X = Page Fault, ✓ = Page Hit)

As shown from the table above, there are 10 page faults. 

Here’s a chronological textual explanation:

1-3. The first three references (“A”, “B”, “C”) are page faults because these pages were not in the memory when they were first referenced, so they need to be loaded into memory.

1. On the next reference “B”, this is already in memory.
2. On the next reference “A”, this is already in memory.
3. On the next reference “D”, this is a page fault because it is not in memory. According to the LRU policy, “C”, which was the least recently used page, is replaced. Now the frames contain “A”, “B”, “D”.
4. On the next reference “A”, this is already in memory.
5. On the next reference “B”, this is already in memory.
6. On the next reference “C”, this is a page fault because it is not in memory. According to the LRU policy, “D”, which was the least recently used page, is replaced. Now the frames contain “A”, “B”, “C”.
7. On the next reference “D”, this is a page fault because it is not in memory. According to the LRU policy, “A”, which was the least recently used page, is replaced. Now the frames contain “D”, “B”, “C”.
8. On the next reference “A”, this is a page fault because it is not in memory. According to the LRU policy, “B”, which was the least recently used page, is replaced. Now the frames contain “D”, “A”, “C”.
9. On the next reference “B”, this is a page fault because it is not in memory. According to the LRU policy, “C”, which was the least recently used page, is replaced. Now the frames contain “D”, “A”, “B”.
10. On the next reference “B”, this is already in memory.
11. On the next reference “C”, this is a page fault because it is not in memory. According to the LRU policy, “D”, which was the least recently used page, is replaced. Now the frames contain “C”, “A”, “B”.
12. On the next reference “B”, this is already in memory.
13. On the next reference “D”, this is a page fault because it is not in memory. According to the LRU policy, “A”, which was the least recently used page, is replaced. Now the frames contain “C”, “D”, “B”.

Therefore, there are 10 page faults.